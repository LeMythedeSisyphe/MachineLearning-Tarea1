{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.exalumnos.usm.cl/wp-content/uploads/2015/06/Isotipo-Negro.gif\" title=\"Title text\" width=\"20%\" height=\"20%\" />\n",
    "\n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "<h1 align='center'> INF-393 Máquinas de Aprendizaje II-2018 </h1>\n",
    "\n",
    "<H3 align='center'> Tarea 1  </H3>\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "<H3 align='center'> Jorge Contreras Cabrera 201573547-6  </H3>\n",
    "\n",
    "<H3 align='center'> César Quiroz Mansilla 201573578-6   </H3>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"primero\"></a>\n",
    "## 1. Aprendizaje con regresión lineal.\n",
    "\n",
    "El modelo de regresión lineal  es una combinación lineal entre variables independientes para obtener otra variable, dependiente de éstas. Lo cual puede resultar bastante simple, pero, hoy en día, ha podido ser aplicado a varios problemas con buenos resultados, como predicción en finanzas y en medicina. Sin embargo, también puede ser un medio para aplicar un modelo más grande, por ejemplo utilizarlo para que, con el resuido, detectar *outliers*, rellenar vacíos/datos incompletos o aprender un *score* para ranquear objetos, lo que haremos en esta sección.\n",
    "\n",
    "<img src=\"http://chanakya.ca/wp-content/uploads/2018/05/EstimateMultipleLinearRegressionCoefficientsExample_01.png\" height=\"15%\" />\n",
    "\n",
    "\n",
    "El problema de *learning to rank* es aplicado comúnmente en *Information Retrieval* (IR). Sin embargo, el aprender ésta función puede ser crucial para modelar la importancia de distintos objetos.  \n",
    "\n",
    "\n",
    "En esta actividad trabajaremos con el problema de predecir el *ranking* mundial de una Universidad en base a distintas características de ésta (dataset *World University Rankings*, a través del siguiente __[link](https://www.kaggle.com/mylesoneill/world-university-rankings)__) en la plataforma de *Kaggle*. En este problema el *ranking* es una medición de qué tan buena es la universidad e intentaremos predecirla a través un modelo simple de regresión lineal. En particular, dentro de los miles de diferentes sistemas de rankings, nacionales e internacionales, entre los cuales comúnmente existen desacuerdos entre ellos, trabajaremos con el ranking ampliamente considerado como uno de las más influyentes y ampliamente observadas: *Times Higher Education World University* .\n",
    "\n",
    "> a) Cargue los datos a analizar, descargándolos desde la plataforma como se indicó, en formato *dataframe pandas*. Descríbalos adecuadamente, ya sea la variable dependiente o las independientes, si es que lo son."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>world_rank</th>\n",
       "      <th>university_name</th>\n",
       "      <th>country</th>\n",
       "      <th>teaching</th>\n",
       "      <th>international</th>\n",
       "      <th>research</th>\n",
       "      <th>citations</th>\n",
       "      <th>income</th>\n",
       "      <th>total_score</th>\n",
       "      <th>num_students</th>\n",
       "      <th>student_staff_ratio</th>\n",
       "      <th>international_students</th>\n",
       "      <th>female_male_ratio</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Harvard University</td>\n",
       "      <td>United States of America</td>\n",
       "      <td>99.7</td>\n",
       "      <td>72.4</td>\n",
       "      <td>98.7</td>\n",
       "      <td>98.8</td>\n",
       "      <td>34.5</td>\n",
       "      <td>96.1</td>\n",
       "      <td>20,152</td>\n",
       "      <td>8.9</td>\n",
       "      <td>25%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>California Institute of Technology</td>\n",
       "      <td>United States of America</td>\n",
       "      <td>97.7</td>\n",
       "      <td>54.6</td>\n",
       "      <td>98.0</td>\n",
       "      <td>99.9</td>\n",
       "      <td>83.7</td>\n",
       "      <td>96.0</td>\n",
       "      <td>2,243</td>\n",
       "      <td>6.9</td>\n",
       "      <td>27%</td>\n",
       "      <td>33 : 67</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Massachusetts Institute of Technology</td>\n",
       "      <td>United States of America</td>\n",
       "      <td>97.8</td>\n",
       "      <td>82.3</td>\n",
       "      <td>91.4</td>\n",
       "      <td>99.9</td>\n",
       "      <td>87.5</td>\n",
       "      <td>95.6</td>\n",
       "      <td>11,074</td>\n",
       "      <td>9.0</td>\n",
       "      <td>33%</td>\n",
       "      <td>37 : 63</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Stanford University</td>\n",
       "      <td>United States of America</td>\n",
       "      <td>98.3</td>\n",
       "      <td>29.5</td>\n",
       "      <td>98.1</td>\n",
       "      <td>99.2</td>\n",
       "      <td>64.3</td>\n",
       "      <td>94.3</td>\n",
       "      <td>15,596</td>\n",
       "      <td>7.8</td>\n",
       "      <td>22%</td>\n",
       "      <td>42 : 58</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Princeton University</td>\n",
       "      <td>United States of America</td>\n",
       "      <td>90.9</td>\n",
       "      <td>70.3</td>\n",
       "      <td>95.4</td>\n",
       "      <td>99.9</td>\n",
       "      <td>-</td>\n",
       "      <td>94.2</td>\n",
       "      <td>7,929</td>\n",
       "      <td>8.4</td>\n",
       "      <td>27%</td>\n",
       "      <td>45 : 55</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>University of Cambridge</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>90.5</td>\n",
       "      <td>77.7</td>\n",
       "      <td>94.1</td>\n",
       "      <td>94.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>91.2</td>\n",
       "      <td>18,812</td>\n",
       "      <td>11.8</td>\n",
       "      <td>34%</td>\n",
       "      <td>46 : 54</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>University of Oxford</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>88.2</td>\n",
       "      <td>77.2</td>\n",
       "      <td>93.9</td>\n",
       "      <td>95.1</td>\n",
       "      <td>73.5</td>\n",
       "      <td>91.2</td>\n",
       "      <td>19,919</td>\n",
       "      <td>11.6</td>\n",
       "      <td>34%</td>\n",
       "      <td>46 : 54</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>University of California, Berkeley</td>\n",
       "      <td>United States of America</td>\n",
       "      <td>84.2</td>\n",
       "      <td>39.6</td>\n",
       "      <td>99.3</td>\n",
       "      <td>97.8</td>\n",
       "      <td>-</td>\n",
       "      <td>91.1</td>\n",
       "      <td>36,186</td>\n",
       "      <td>16.4</td>\n",
       "      <td>15%</td>\n",
       "      <td>50 : 50</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Imperial College London</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>89.2</td>\n",
       "      <td>90.0</td>\n",
       "      <td>94.5</td>\n",
       "      <td>88.3</td>\n",
       "      <td>92.9</td>\n",
       "      <td>90.6</td>\n",
       "      <td>15,060</td>\n",
       "      <td>11.7</td>\n",
       "      <td>51%</td>\n",
       "      <td>37 : 63</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Yale University</td>\n",
       "      <td>United States of America</td>\n",
       "      <td>92.1</td>\n",
       "      <td>59.2</td>\n",
       "      <td>89.7</td>\n",
       "      <td>91.5</td>\n",
       "      <td>-</td>\n",
       "      <td>89.5</td>\n",
       "      <td>11,751</td>\n",
       "      <td>4.4</td>\n",
       "      <td>20%</td>\n",
       "      <td>50 : 50</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>University of California, Los Angeles</td>\n",
       "      <td>United States of America</td>\n",
       "      <td>83.0</td>\n",
       "      <td>48.1</td>\n",
       "      <td>92.9</td>\n",
       "      <td>93.2</td>\n",
       "      <td>-</td>\n",
       "      <td>87.7</td>\n",
       "      <td>38,206</td>\n",
       "      <td>10.3</td>\n",
       "      <td>15%</td>\n",
       "      <td>52 : 48</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>University of Chicago</td>\n",
       "      <td>United States of America</td>\n",
       "      <td>79.1</td>\n",
       "      <td>62.8</td>\n",
       "      <td>87.9</td>\n",
       "      <td>96.9</td>\n",
       "      <td>-</td>\n",
       "      <td>86.9</td>\n",
       "      <td>14,221</td>\n",
       "      <td>6.9</td>\n",
       "      <td>21%</td>\n",
       "      <td>42 : 58</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>Johns Hopkins University</td>\n",
       "      <td>United States of America</td>\n",
       "      <td>80.9</td>\n",
       "      <td>58.5</td>\n",
       "      <td>89.2</td>\n",
       "      <td>92.3</td>\n",
       "      <td>100.0</td>\n",
       "      <td>86.4</td>\n",
       "      <td>15,128</td>\n",
       "      <td>3.6</td>\n",
       "      <td>23%</td>\n",
       "      <td>50 : 50</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>Cornell University</td>\n",
       "      <td>United States of America</td>\n",
       "      <td>82.2</td>\n",
       "      <td>62.4</td>\n",
       "      <td>88.8</td>\n",
       "      <td>88.1</td>\n",
       "      <td>34.7</td>\n",
       "      <td>83.9</td>\n",
       "      <td>21,424</td>\n",
       "      <td>10.2</td>\n",
       "      <td>19%</td>\n",
       "      <td>48 : 52</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>ETH Zurich – Swiss Federal Institute of Techno...</td>\n",
       "      <td>Switzerland</td>\n",
       "      <td>77.5</td>\n",
       "      <td>93.7</td>\n",
       "      <td>87.8</td>\n",
       "      <td>83.1</td>\n",
       "      <td>-</td>\n",
       "      <td>83.4</td>\n",
       "      <td>18,178</td>\n",
       "      <td>14.7</td>\n",
       "      <td>37%</td>\n",
       "      <td>31 : 69</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>University of Michigan</td>\n",
       "      <td>United States of America</td>\n",
       "      <td>83.9</td>\n",
       "      <td>53.3</td>\n",
       "      <td>89.1</td>\n",
       "      <td>84.1</td>\n",
       "      <td>59.6</td>\n",
       "      <td>83.4</td>\n",
       "      <td>41,786</td>\n",
       "      <td>9.0</td>\n",
       "      <td>16%</td>\n",
       "      <td>48 : 52</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>University of Toronto</td>\n",
       "      <td>Canada</td>\n",
       "      <td>75.8</td>\n",
       "      <td>-</td>\n",
       "      <td>87.9</td>\n",
       "      <td>82.2</td>\n",
       "      <td>-</td>\n",
       "      <td>82.0</td>\n",
       "      <td>66,198</td>\n",
       "      <td>19.5</td>\n",
       "      <td>15%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>Columbia University</td>\n",
       "      <td>United States of America</td>\n",
       "      <td>73.8</td>\n",
       "      <td>90.9</td>\n",
       "      <td>73.8</td>\n",
       "      <td>92.6</td>\n",
       "      <td>-</td>\n",
       "      <td>81.0</td>\n",
       "      <td>25,055</td>\n",
       "      <td>5.9</td>\n",
       "      <td>28%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>University of Pennsylvania</td>\n",
       "      <td>United States of America</td>\n",
       "      <td>71.8</td>\n",
       "      <td>32.9</td>\n",
       "      <td>82.7</td>\n",
       "      <td>93.6</td>\n",
       "      <td>43.7</td>\n",
       "      <td>79.5</td>\n",
       "      <td>20,376</td>\n",
       "      <td>6.5</td>\n",
       "      <td>20%</td>\n",
       "      <td>51 : 49</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>Carnegie Mellon University</td>\n",
       "      <td>United States of America</td>\n",
       "      <td>70.3</td>\n",
       "      <td>39.1</td>\n",
       "      <td>79.3</td>\n",
       "      <td>95.7</td>\n",
       "      <td>53.7</td>\n",
       "      <td>79.3</td>\n",
       "      <td>11,885</td>\n",
       "      <td>13.1</td>\n",
       "      <td>35%</td>\n",
       "      <td>39 : 61</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>University of Hong Kong</td>\n",
       "      <td>Hong Kong</td>\n",
       "      <td>68.4</td>\n",
       "      <td>91.4</td>\n",
       "      <td>71.4</td>\n",
       "      <td>96.1</td>\n",
       "      <td>56.5</td>\n",
       "      <td>79.2</td>\n",
       "      <td>19,835</td>\n",
       "      <td>17.6</td>\n",
       "      <td>38%</td>\n",
       "      <td>53 : 47</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>University College London</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>74.0</td>\n",
       "      <td>90.8</td>\n",
       "      <td>81.6</td>\n",
       "      <td>80.6</td>\n",
       "      <td>39.0</td>\n",
       "      <td>78.4</td>\n",
       "      <td>26,607</td>\n",
       "      <td>10.7</td>\n",
       "      <td>46%</td>\n",
       "      <td>56 : 44</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>University of Washington</td>\n",
       "      <td>United States of America</td>\n",
       "      <td>68.2</td>\n",
       "      <td>49.0</td>\n",
       "      <td>77.1</td>\n",
       "      <td>95.9</td>\n",
       "      <td>32.8</td>\n",
       "      <td>78.0</td>\n",
       "      <td>44,020</td>\n",
       "      <td>11.8</td>\n",
       "      <td>13%</td>\n",
       "      <td>53 : 47</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>Duke University</td>\n",
       "      <td>United States of America</td>\n",
       "      <td>66.8</td>\n",
       "      <td>49.4</td>\n",
       "      <td>71.5</td>\n",
       "      <td>92.3</td>\n",
       "      <td>100.0</td>\n",
       "      <td>76.5</td>\n",
       "      <td>15,172</td>\n",
       "      <td>4.8</td>\n",
       "      <td>17%</td>\n",
       "      <td>49 : 51</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>Northwestern University</td>\n",
       "      <td>United States of America</td>\n",
       "      <td>64.5</td>\n",
       "      <td>60.5</td>\n",
       "      <td>68.8</td>\n",
       "      <td>95.3</td>\n",
       "      <td>-</td>\n",
       "      <td>75.9</td>\n",
       "      <td>18,334</td>\n",
       "      <td>13.8</td>\n",
       "      <td>15%</td>\n",
       "      <td>48 : 52</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>University of Tokyo</td>\n",
       "      <td>Japan</td>\n",
       "      <td>87.7</td>\n",
       "      <td>18.4</td>\n",
       "      <td>91.9</td>\n",
       "      <td>58.1</td>\n",
       "      <td>-</td>\n",
       "      <td>75.6</td>\n",
       "      <td>26,199</td>\n",
       "      <td>5.7</td>\n",
       "      <td>10%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>Georgia Institute of Technology</td>\n",
       "      <td>United States of America</td>\n",
       "      <td>67.9</td>\n",
       "      <td>73.2</td>\n",
       "      <td>72.6</td>\n",
       "      <td>83.2</td>\n",
       "      <td>95.1</td>\n",
       "      <td>75.3</td>\n",
       "      <td>19,967</td>\n",
       "      <td>20.1</td>\n",
       "      <td>26%</td>\n",
       "      <td>31 : 69</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>Pohang University of Science and Technology</td>\n",
       "      <td>South Korea</td>\n",
       "      <td>69.5</td>\n",
       "      <td>32.6</td>\n",
       "      <td>62.5</td>\n",
       "      <td>96.5</td>\n",
       "      <td>100.0</td>\n",
       "      <td>75.1</td>\n",
       "      <td>3,055</td>\n",
       "      <td>10.1</td>\n",
       "      <td>4%</td>\n",
       "      <td>20 : 80</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>University of California, Santa Barbara</td>\n",
       "      <td>United States of America</td>\n",
       "      <td>56.6</td>\n",
       "      <td>64.3</td>\n",
       "      <td>68.0</td>\n",
       "      <td>98.8</td>\n",
       "      <td>89.8</td>\n",
       "      <td>75.0</td>\n",
       "      <td>22,020</td>\n",
       "      <td>27.3</td>\n",
       "      <td>11%</td>\n",
       "      <td>52 : 48</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>University of British Columbia</td>\n",
       "      <td>Canada</td>\n",
       "      <td>65.1</td>\n",
       "      <td>93.3</td>\n",
       "      <td>74.8</td>\n",
       "      <td>80.3</td>\n",
       "      <td>42.6</td>\n",
       "      <td>73.8</td>\n",
       "      <td>50,152</td>\n",
       "      <td>17.6</td>\n",
       "      <td>25%</td>\n",
       "      <td>54 : 46</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2573</th>\n",
       "      <td>601-800</td>\n",
       "      <td>University of Tehran</td>\n",
       "      <td>Iran</td>\n",
       "      <td>26.1</td>\n",
       "      <td>16.5</td>\n",
       "      <td>16.9</td>\n",
       "      <td>15.8</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>53,802</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1%</td>\n",
       "      <td>45 : 55</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2574</th>\n",
       "      <td>601-800</td>\n",
       "      <td>University of Texas at El Paso</td>\n",
       "      <td>United States of America</td>\n",
       "      <td>18.6</td>\n",
       "      <td>30.4</td>\n",
       "      <td>18.7</td>\n",
       "      <td>18.4</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>19,123</td>\n",
       "      <td>29.0</td>\n",
       "      <td>7%</td>\n",
       "      <td>54 : 46</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2575</th>\n",
       "      <td>601-800</td>\n",
       "      <td>Texas Tech University</td>\n",
       "      <td>United States of America</td>\n",
       "      <td>27.9</td>\n",
       "      <td>36.8</td>\n",
       "      <td>17.2</td>\n",
       "      <td>22.0</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>29,512</td>\n",
       "      <td>20.9</td>\n",
       "      <td>7%</td>\n",
       "      <td>46 : 54</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2576</th>\n",
       "      <td>601-800</td>\n",
       "      <td>Tokai University</td>\n",
       "      <td>Japan</td>\n",
       "      <td>17.9</td>\n",
       "      <td>19.3</td>\n",
       "      <td>7.6</td>\n",
       "      <td>15.3</td>\n",
       "      <td>34.4</td>\n",
       "      <td>-</td>\n",
       "      <td>29,700</td>\n",
       "      <td>12.7</td>\n",
       "      <td>1%</td>\n",
       "      <td>27 : 73</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2577</th>\n",
       "      <td>601-800</td>\n",
       "      <td>Tokushima University</td>\n",
       "      <td>Japan</td>\n",
       "      <td>25.3</td>\n",
       "      <td>16.8</td>\n",
       "      <td>21.6</td>\n",
       "      <td>12.8</td>\n",
       "      <td>59.6</td>\n",
       "      <td>-</td>\n",
       "      <td>7,519</td>\n",
       "      <td>8.9</td>\n",
       "      <td>3%</td>\n",
       "      <td>34 : 66</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2578</th>\n",
       "      <td>601-800</td>\n",
       "      <td>Tokyo University of Marine Science and Technology</td>\n",
       "      <td>Japan</td>\n",
       "      <td>27.9</td>\n",
       "      <td>24.5</td>\n",
       "      <td>12.4</td>\n",
       "      <td>7.7</td>\n",
       "      <td>57.9</td>\n",
       "      <td>-</td>\n",
       "      <td>2,597</td>\n",
       "      <td>11.1</td>\n",
       "      <td>7%</td>\n",
       "      <td>34 : 66</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2579</th>\n",
       "      <td>601-800</td>\n",
       "      <td>Tokyo University of Science</td>\n",
       "      <td>Japan</td>\n",
       "      <td>23.0</td>\n",
       "      <td>15.4</td>\n",
       "      <td>24.1</td>\n",
       "      <td>21.4</td>\n",
       "      <td>37.6</td>\n",
       "      <td>-</td>\n",
       "      <td>20,243</td>\n",
       "      <td>25.7</td>\n",
       "      <td>2%</td>\n",
       "      <td>20 : 80</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2580</th>\n",
       "      <td>601-800</td>\n",
       "      <td>Tomsk State University</td>\n",
       "      <td>Russian Federation</td>\n",
       "      <td>34.8</td>\n",
       "      <td>36.9</td>\n",
       "      <td>20.8</td>\n",
       "      <td>7.6</td>\n",
       "      <td>44.0</td>\n",
       "      <td>-</td>\n",
       "      <td>10,413</td>\n",
       "      <td>9.9</td>\n",
       "      <td>12%</td>\n",
       "      <td>60 : 40</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2581</th>\n",
       "      <td>601-800</td>\n",
       "      <td>Tottori University</td>\n",
       "      <td>Japan</td>\n",
       "      <td>24.3</td>\n",
       "      <td>16.7</td>\n",
       "      <td>10.1</td>\n",
       "      <td>9.6</td>\n",
       "      <td>34.5</td>\n",
       "      <td>-</td>\n",
       "      <td>6,248</td>\n",
       "      <td>8.2</td>\n",
       "      <td>2%</td>\n",
       "      <td>34 : 66</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2582</th>\n",
       "      <td>601-800</td>\n",
       "      <td>Toyohashi University of Technology</td>\n",
       "      <td>Japan</td>\n",
       "      <td>22.0</td>\n",
       "      <td>25.4</td>\n",
       "      <td>18.9</td>\n",
       "      <td>15.8</td>\n",
       "      <td>50.3</td>\n",
       "      <td>-</td>\n",
       "      <td>2,153</td>\n",
       "      <td>9.3</td>\n",
       "      <td>9%</td>\n",
       "      <td>9 : 91</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2583</th>\n",
       "      <td>601-800</td>\n",
       "      <td>Universiti Kebangsaan Malaysia</td>\n",
       "      <td>Malaysia</td>\n",
       "      <td>24.3</td>\n",
       "      <td>29.7</td>\n",
       "      <td>15.9</td>\n",
       "      <td>10.9</td>\n",
       "      <td>28.4</td>\n",
       "      <td>-</td>\n",
       "      <td>24,227</td>\n",
       "      <td>11.8</td>\n",
       "      <td>12%</td>\n",
       "      <td>62 : 38</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2584</th>\n",
       "      <td>601-800</td>\n",
       "      <td>Universiti Putra Malaysia</td>\n",
       "      <td>Malaysia</td>\n",
       "      <td>25.3</td>\n",
       "      <td>50.1</td>\n",
       "      <td>20.9</td>\n",
       "      <td>10.2</td>\n",
       "      <td>34.2</td>\n",
       "      <td>-</td>\n",
       "      <td>23,883</td>\n",
       "      <td>12.2</td>\n",
       "      <td>16%</td>\n",
       "      <td>63 : 37</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2585</th>\n",
       "      <td>601-800</td>\n",
       "      <td>Universiti Sains Malaysia</td>\n",
       "      <td>Malaysia</td>\n",
       "      <td>26.9</td>\n",
       "      <td>44.2</td>\n",
       "      <td>16.6</td>\n",
       "      <td>12.4</td>\n",
       "      <td>34.4</td>\n",
       "      <td>-</td>\n",
       "      <td>28,179</td>\n",
       "      <td>14.8</td>\n",
       "      <td>10%</td>\n",
       "      <td>61 : 39</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2586</th>\n",
       "      <td>601-800</td>\n",
       "      <td>Universiti Teknologi MARA</td>\n",
       "      <td>Malaysia</td>\n",
       "      <td>15.2</td>\n",
       "      <td>14.8</td>\n",
       "      <td>7.7</td>\n",
       "      <td>18.2</td>\n",
       "      <td>28.3</td>\n",
       "      <td>-</td>\n",
       "      <td>69,268</td>\n",
       "      <td>16.8</td>\n",
       "      <td>0%</td>\n",
       "      <td>65 : 35</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2587</th>\n",
       "      <td>601-800</td>\n",
       "      <td>Ural Federal University</td>\n",
       "      <td>Russian Federation</td>\n",
       "      <td>24.8</td>\n",
       "      <td>17.3</td>\n",
       "      <td>10.6</td>\n",
       "      <td>16.8</td>\n",
       "      <td>35.6</td>\n",
       "      <td>-</td>\n",
       "      <td>28,427</td>\n",
       "      <td>10.1</td>\n",
       "      <td>3%</td>\n",
       "      <td>48 : 52</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2588</th>\n",
       "      <td>601-800</td>\n",
       "      <td>V.N. Karazin Kharkiv National University</td>\n",
       "      <td>Ukraine</td>\n",
       "      <td>21.7</td>\n",
       "      <td>48.4</td>\n",
       "      <td>8.9</td>\n",
       "      <td>1.7</td>\n",
       "      <td>28.8</td>\n",
       "      <td>-</td>\n",
       "      <td>14,410</td>\n",
       "      <td>9.7</td>\n",
       "      <td>22%</td>\n",
       "      <td>53 : 47</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2589</th>\n",
       "      <td>601-800</td>\n",
       "      <td>University of Vigo</td>\n",
       "      <td>Spain</td>\n",
       "      <td>18.4</td>\n",
       "      <td>30.7</td>\n",
       "      <td>10.5</td>\n",
       "      <td>31.8</td>\n",
       "      <td>38.1</td>\n",
       "      <td>-</td>\n",
       "      <td>22,793</td>\n",
       "      <td>19.0</td>\n",
       "      <td>3%</td>\n",
       "      <td>51 : 49</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2590</th>\n",
       "      <td>601-800</td>\n",
       "      <td>Vilnius University</td>\n",
       "      <td>Lithuania</td>\n",
       "      <td>18.3</td>\n",
       "      <td>40.8</td>\n",
       "      <td>13.6</td>\n",
       "      <td>26.1</td>\n",
       "      <td>41.0</td>\n",
       "      <td>-</td>\n",
       "      <td>19,019</td>\n",
       "      <td>14.2</td>\n",
       "      <td>4%</td>\n",
       "      <td>65 : 35</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2591</th>\n",
       "      <td>601-800</td>\n",
       "      <td>Warsaw University of Technology</td>\n",
       "      <td>Poland</td>\n",
       "      <td>19.4</td>\n",
       "      <td>20.7</td>\n",
       "      <td>8.5</td>\n",
       "      <td>40.3</td>\n",
       "      <td>47.4</td>\n",
       "      <td>-</td>\n",
       "      <td>34,572</td>\n",
       "      <td>14.5</td>\n",
       "      <td>3%</td>\n",
       "      <td>34 : 66</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2592</th>\n",
       "      <td>601-800</td>\n",
       "      <td>Waseda University</td>\n",
       "      <td>Japan</td>\n",
       "      <td>23.6</td>\n",
       "      <td>29.7</td>\n",
       "      <td>14.6</td>\n",
       "      <td>29.4</td>\n",
       "      <td>32.4</td>\n",
       "      <td>-</td>\n",
       "      <td>52,316</td>\n",
       "      <td>16.9</td>\n",
       "      <td>8%</td>\n",
       "      <td>35 : 65</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2593</th>\n",
       "      <td>601-800</td>\n",
       "      <td>University of West Bohemia</td>\n",
       "      <td>Czech Republic</td>\n",
       "      <td>16.3</td>\n",
       "      <td>23.1</td>\n",
       "      <td>9.7</td>\n",
       "      <td>29.8</td>\n",
       "      <td>32.1</td>\n",
       "      <td>-</td>\n",
       "      <td>15,639</td>\n",
       "      <td>21.5</td>\n",
       "      <td>2%</td>\n",
       "      <td>52 : 48</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2594</th>\n",
       "      <td>601-800</td>\n",
       "      <td>University of the West of England</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>16.9</td>\n",
       "      <td>48.5</td>\n",
       "      <td>11.2</td>\n",
       "      <td>34.6</td>\n",
       "      <td>28.5</td>\n",
       "      <td>-</td>\n",
       "      <td>22,525</td>\n",
       "      <td>21.4</td>\n",
       "      <td>15%</td>\n",
       "      <td>53 : 47</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2595</th>\n",
       "      <td>601-800</td>\n",
       "      <td>West University of Timişoara</td>\n",
       "      <td>Romania</td>\n",
       "      <td>16.1</td>\n",
       "      <td>21.0</td>\n",
       "      <td>3.9</td>\n",
       "      <td>22.4</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>12,933</td>\n",
       "      <td>19.0</td>\n",
       "      <td>3%</td>\n",
       "      <td>62 : 38</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2596</th>\n",
       "      <td>601-800</td>\n",
       "      <td>University of Westminster</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>17.3</td>\n",
       "      <td>81.9</td>\n",
       "      <td>11.7</td>\n",
       "      <td>21.1</td>\n",
       "      <td>28.5</td>\n",
       "      <td>-</td>\n",
       "      <td>16,609</td>\n",
       "      <td>21.0</td>\n",
       "      <td>43%</td>\n",
       "      <td>57 : 43</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2597</th>\n",
       "      <td>601-800</td>\n",
       "      <td>Xidian University</td>\n",
       "      <td>China</td>\n",
       "      <td>17.9</td>\n",
       "      <td>12.8</td>\n",
       "      <td>12.1</td>\n",
       "      <td>8.9</td>\n",
       "      <td>83.7</td>\n",
       "      <td>-</td>\n",
       "      <td>31,618</td>\n",
       "      <td>16.4</td>\n",
       "      <td>2%</td>\n",
       "      <td>29 : 71</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2598</th>\n",
       "      <td>601-800</td>\n",
       "      <td>Yeungnam University</td>\n",
       "      <td>South Korea</td>\n",
       "      <td>18.6</td>\n",
       "      <td>24.3</td>\n",
       "      <td>10.9</td>\n",
       "      <td>26.5</td>\n",
       "      <td>35.4</td>\n",
       "      <td>-</td>\n",
       "      <td>21,958</td>\n",
       "      <td>15.3</td>\n",
       "      <td>3%</td>\n",
       "      <td>48 : 52</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2599</th>\n",
       "      <td>601-800</td>\n",
       "      <td>Yıldız Technical University</td>\n",
       "      <td>Turkey</td>\n",
       "      <td>14.5</td>\n",
       "      <td>14.9</td>\n",
       "      <td>7.6</td>\n",
       "      <td>19.3</td>\n",
       "      <td>44.0</td>\n",
       "      <td>-</td>\n",
       "      <td>31,268</td>\n",
       "      <td>28.7</td>\n",
       "      <td>2%</td>\n",
       "      <td>36 : 64</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2600</th>\n",
       "      <td>601-800</td>\n",
       "      <td>Yokohama City University</td>\n",
       "      <td>Japan</td>\n",
       "      <td>24.0</td>\n",
       "      <td>16.1</td>\n",
       "      <td>10.2</td>\n",
       "      <td>36.4</td>\n",
       "      <td>37.9</td>\n",
       "      <td>-</td>\n",
       "      <td>4,122</td>\n",
       "      <td>3.7</td>\n",
       "      <td>3%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2601</th>\n",
       "      <td>601-800</td>\n",
       "      <td>Yokohama National University</td>\n",
       "      <td>Japan</td>\n",
       "      <td>20.1</td>\n",
       "      <td>23.3</td>\n",
       "      <td>16.0</td>\n",
       "      <td>13.5</td>\n",
       "      <td>40.4</td>\n",
       "      <td>-</td>\n",
       "      <td>10,117</td>\n",
       "      <td>12.1</td>\n",
       "      <td>8%</td>\n",
       "      <td>28 : 72</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2602</th>\n",
       "      <td>601-800</td>\n",
       "      <td>Yuan Ze University</td>\n",
       "      <td>Taiwan</td>\n",
       "      <td>16.2</td>\n",
       "      <td>17.7</td>\n",
       "      <td>18.3</td>\n",
       "      <td>28.6</td>\n",
       "      <td>39.8</td>\n",
       "      <td>-</td>\n",
       "      <td>8,663</td>\n",
       "      <td>20.6</td>\n",
       "      <td>4%</td>\n",
       "      <td>43 : 57</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2603 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     world_rank                                    university_name  \\\n",
       "0             1                                 Harvard University   \n",
       "1             2                 California Institute of Technology   \n",
       "2             3              Massachusetts Institute of Technology   \n",
       "3             4                                Stanford University   \n",
       "4             5                               Princeton University   \n",
       "5             6                            University of Cambridge   \n",
       "6             6                               University of Oxford   \n",
       "7             8                 University of California, Berkeley   \n",
       "8             9                            Imperial College London   \n",
       "9            10                                    Yale University   \n",
       "10           11              University of California, Los Angeles   \n",
       "11           12                              University of Chicago   \n",
       "12           13                           Johns Hopkins University   \n",
       "13           14                                 Cornell University   \n",
       "14           15  ETH Zurich – Swiss Federal Institute of Techno...   \n",
       "15           15                             University of Michigan   \n",
       "16           17                              University of Toronto   \n",
       "17           18                                Columbia University   \n",
       "18           19                         University of Pennsylvania   \n",
       "19           20                         Carnegie Mellon University   \n",
       "20           21                            University of Hong Kong   \n",
       "21           22                          University College London   \n",
       "22           23                           University of Washington   \n",
       "23           24                                    Duke University   \n",
       "24           25                            Northwestern University   \n",
       "25           26                                University of Tokyo   \n",
       "26           27                    Georgia Institute of Technology   \n",
       "27           28        Pohang University of Science and Technology   \n",
       "28           29            University of California, Santa Barbara   \n",
       "29           30                     University of British Columbia   \n",
       "...         ...                                                ...   \n",
       "2573    601-800                               University of Tehran   \n",
       "2574    601-800                     University of Texas at El Paso   \n",
       "2575    601-800                              Texas Tech University   \n",
       "2576    601-800                                   Tokai University   \n",
       "2577    601-800                               Tokushima University   \n",
       "2578    601-800  Tokyo University of Marine Science and Technology   \n",
       "2579    601-800                        Tokyo University of Science   \n",
       "2580    601-800                             Tomsk State University   \n",
       "2581    601-800                                 Tottori University   \n",
       "2582    601-800                 Toyohashi University of Technology   \n",
       "2583    601-800                     Universiti Kebangsaan Malaysia   \n",
       "2584    601-800                          Universiti Putra Malaysia   \n",
       "2585    601-800                          Universiti Sains Malaysia   \n",
       "2586    601-800                          Universiti Teknologi MARA   \n",
       "2587    601-800                            Ural Federal University   \n",
       "2588    601-800           V.N. Karazin Kharkiv National University   \n",
       "2589    601-800                                 University of Vigo   \n",
       "2590    601-800                                 Vilnius University   \n",
       "2591    601-800                    Warsaw University of Technology   \n",
       "2592    601-800                                  Waseda University   \n",
       "2593    601-800                         University of West Bohemia   \n",
       "2594    601-800                  University of the West of England   \n",
       "2595    601-800                       West University of Timişoara   \n",
       "2596    601-800                          University of Westminster   \n",
       "2597    601-800                                  Xidian University   \n",
       "2598    601-800                                Yeungnam University   \n",
       "2599    601-800                        Yıldız Technical University   \n",
       "2600    601-800                           Yokohama City University   \n",
       "2601    601-800                       Yokohama National University   \n",
       "2602    601-800                                 Yuan Ze University   \n",
       "\n",
       "                       country  teaching international  research  citations  \\\n",
       "0     United States of America      99.7          72.4      98.7       98.8   \n",
       "1     United States of America      97.7          54.6      98.0       99.9   \n",
       "2     United States of America      97.8          82.3      91.4       99.9   \n",
       "3     United States of America      98.3          29.5      98.1       99.2   \n",
       "4     United States of America      90.9          70.3      95.4       99.9   \n",
       "5               United Kingdom      90.5          77.7      94.1       94.0   \n",
       "6               United Kingdom      88.2          77.2      93.9       95.1   \n",
       "7     United States of America      84.2          39.6      99.3       97.8   \n",
       "8               United Kingdom      89.2          90.0      94.5       88.3   \n",
       "9     United States of America      92.1          59.2      89.7       91.5   \n",
       "10    United States of America      83.0          48.1      92.9       93.2   \n",
       "11    United States of America      79.1          62.8      87.9       96.9   \n",
       "12    United States of America      80.9          58.5      89.2       92.3   \n",
       "13    United States of America      82.2          62.4      88.8       88.1   \n",
       "14                 Switzerland      77.5          93.7      87.8       83.1   \n",
       "15    United States of America      83.9          53.3      89.1       84.1   \n",
       "16                      Canada      75.8             -      87.9       82.2   \n",
       "17    United States of America      73.8          90.9      73.8       92.6   \n",
       "18    United States of America      71.8          32.9      82.7       93.6   \n",
       "19    United States of America      70.3          39.1      79.3       95.7   \n",
       "20                   Hong Kong      68.4          91.4      71.4       96.1   \n",
       "21              United Kingdom      74.0          90.8      81.6       80.6   \n",
       "22    United States of America      68.2          49.0      77.1       95.9   \n",
       "23    United States of America      66.8          49.4      71.5       92.3   \n",
       "24    United States of America      64.5          60.5      68.8       95.3   \n",
       "25                       Japan      87.7          18.4      91.9       58.1   \n",
       "26    United States of America      67.9          73.2      72.6       83.2   \n",
       "27                 South Korea      69.5          32.6      62.5       96.5   \n",
       "28    United States of America      56.6          64.3      68.0       98.8   \n",
       "29                      Canada      65.1          93.3      74.8       80.3   \n",
       "...                        ...       ...           ...       ...        ...   \n",
       "2573                      Iran      26.1          16.5      16.9       15.8   \n",
       "2574  United States of America      18.6          30.4      18.7       18.4   \n",
       "2575  United States of America      27.9          36.8      17.2       22.0   \n",
       "2576                     Japan      17.9          19.3       7.6       15.3   \n",
       "2577                     Japan      25.3          16.8      21.6       12.8   \n",
       "2578                     Japan      27.9          24.5      12.4        7.7   \n",
       "2579                     Japan      23.0          15.4      24.1       21.4   \n",
       "2580        Russian Federation      34.8          36.9      20.8        7.6   \n",
       "2581                     Japan      24.3          16.7      10.1        9.6   \n",
       "2582                     Japan      22.0          25.4      18.9       15.8   \n",
       "2583                  Malaysia      24.3          29.7      15.9       10.9   \n",
       "2584                  Malaysia      25.3          50.1      20.9       10.2   \n",
       "2585                  Malaysia      26.9          44.2      16.6       12.4   \n",
       "2586                  Malaysia      15.2          14.8       7.7       18.2   \n",
       "2587        Russian Federation      24.8          17.3      10.6       16.8   \n",
       "2588                   Ukraine      21.7          48.4       8.9        1.7   \n",
       "2589                     Spain      18.4          30.7      10.5       31.8   \n",
       "2590                 Lithuania      18.3          40.8      13.6       26.1   \n",
       "2591                    Poland      19.4          20.7       8.5       40.3   \n",
       "2592                     Japan      23.6          29.7      14.6       29.4   \n",
       "2593            Czech Republic      16.3          23.1       9.7       29.8   \n",
       "2594            United Kingdom      16.9          48.5      11.2       34.6   \n",
       "2595                   Romania      16.1          21.0       3.9       22.4   \n",
       "2596            United Kingdom      17.3          81.9      11.7       21.1   \n",
       "2597                     China      17.9          12.8      12.1        8.9   \n",
       "2598               South Korea      18.6          24.3      10.9       26.5   \n",
       "2599                    Turkey      14.5          14.9       7.6       19.3   \n",
       "2600                     Japan      24.0          16.1      10.2       36.4   \n",
       "2601                     Japan      20.1          23.3      16.0       13.5   \n",
       "2602                    Taiwan      16.2          17.7      18.3       28.6   \n",
       "\n",
       "     income total_score num_students  student_staff_ratio  \\\n",
       "0      34.5        96.1       20,152                  8.9   \n",
       "1      83.7        96.0        2,243                  6.9   \n",
       "2      87.5        95.6       11,074                  9.0   \n",
       "3      64.3        94.3       15,596                  7.8   \n",
       "4         -        94.2        7,929                  8.4   \n",
       "5      57.0        91.2       18,812                 11.8   \n",
       "6      73.5        91.2       19,919                 11.6   \n",
       "7         -        91.1       36,186                 16.4   \n",
       "8      92.9        90.6       15,060                 11.7   \n",
       "9         -        89.5       11,751                  4.4   \n",
       "10        -        87.7       38,206                 10.3   \n",
       "11        -        86.9       14,221                  6.9   \n",
       "12    100.0        86.4       15,128                  3.6   \n",
       "13     34.7        83.9       21,424                 10.2   \n",
       "14        -        83.4       18,178                 14.7   \n",
       "15     59.6        83.4       41,786                  9.0   \n",
       "16        -        82.0       66,198                 19.5   \n",
       "17        -        81.0       25,055                  5.9   \n",
       "18     43.7        79.5       20,376                  6.5   \n",
       "19     53.7        79.3       11,885                 13.1   \n",
       "20     56.5        79.2       19,835                 17.6   \n",
       "21     39.0        78.4       26,607                 10.7   \n",
       "22     32.8        78.0       44,020                 11.8   \n",
       "23    100.0        76.5       15,172                  4.8   \n",
       "24        -        75.9       18,334                 13.8   \n",
       "25        -        75.6       26,199                  5.7   \n",
       "26     95.1        75.3       19,967                 20.1   \n",
       "27    100.0        75.1        3,055                 10.1   \n",
       "28     89.8        75.0       22,020                 27.3   \n",
       "29     42.6        73.8       50,152                 17.6   \n",
       "...     ...         ...          ...                  ...   \n",
       "2573      -           -       53,802                 27.0   \n",
       "2574      -           -       19,123                 29.0   \n",
       "2575      -           -       29,512                 20.9   \n",
       "2576   34.4           -       29,700                 12.7   \n",
       "2577   59.6           -        7,519                  8.9   \n",
       "2578   57.9           -        2,597                 11.1   \n",
       "2579   37.6           -       20,243                 25.7   \n",
       "2580   44.0           -       10,413                  9.9   \n",
       "2581   34.5           -        6,248                  8.2   \n",
       "2582   50.3           -        2,153                  9.3   \n",
       "2583   28.4           -       24,227                 11.8   \n",
       "2584   34.2           -       23,883                 12.2   \n",
       "2585   34.4           -       28,179                 14.8   \n",
       "2586   28.3           -       69,268                 16.8   \n",
       "2587   35.6           -       28,427                 10.1   \n",
       "2588   28.8           -       14,410                  9.7   \n",
       "2589   38.1           -       22,793                 19.0   \n",
       "2590   41.0           -       19,019                 14.2   \n",
       "2591   47.4           -       34,572                 14.5   \n",
       "2592   32.4           -       52,316                 16.9   \n",
       "2593   32.1           -       15,639                 21.5   \n",
       "2594   28.5           -       22,525                 21.4   \n",
       "2595      -           -       12,933                 19.0   \n",
       "2596   28.5           -       16,609                 21.0   \n",
       "2597   83.7           -       31,618                 16.4   \n",
       "2598   35.4           -       21,958                 15.3   \n",
       "2599   44.0           -       31,268                 28.7   \n",
       "2600   37.9           -        4,122                  3.7   \n",
       "2601   40.4           -       10,117                 12.1   \n",
       "2602   39.8           -        8,663                 20.6   \n",
       "\n",
       "     international_students female_male_ratio  year  \n",
       "0                       25%               NaN  2011  \n",
       "1                       27%           33 : 67  2011  \n",
       "2                       33%           37 : 63  2011  \n",
       "3                       22%           42 : 58  2011  \n",
       "4                       27%           45 : 55  2011  \n",
       "5                       34%           46 : 54  2011  \n",
       "6                       34%           46 : 54  2011  \n",
       "7                       15%           50 : 50  2011  \n",
       "8                       51%           37 : 63  2011  \n",
       "9                       20%           50 : 50  2011  \n",
       "10                      15%           52 : 48  2011  \n",
       "11                      21%           42 : 58  2011  \n",
       "12                      23%           50 : 50  2011  \n",
       "13                      19%           48 : 52  2011  \n",
       "14                      37%           31 : 69  2011  \n",
       "15                      16%           48 : 52  2011  \n",
       "16                      15%               NaN  2011  \n",
       "17                      28%               NaN  2011  \n",
       "18                      20%           51 : 49  2011  \n",
       "19                      35%           39 : 61  2011  \n",
       "20                      38%           53 : 47  2011  \n",
       "21                      46%           56 : 44  2011  \n",
       "22                      13%           53 : 47  2011  \n",
       "23                      17%           49 : 51  2011  \n",
       "24                      15%           48 : 52  2011  \n",
       "25                      10%               NaN  2011  \n",
       "26                      26%           31 : 69  2011  \n",
       "27                       4%           20 : 80  2011  \n",
       "28                      11%           52 : 48  2011  \n",
       "29                      25%           54 : 46  2011  \n",
       "...                     ...               ...   ...  \n",
       "2573                     1%           45 : 55  2016  \n",
       "2574                     7%           54 : 46  2016  \n",
       "2575                     7%           46 : 54  2016  \n",
       "2576                     1%           27 : 73  2016  \n",
       "2577                     3%           34 : 66  2016  \n",
       "2578                     7%           34 : 66  2016  \n",
       "2579                     2%           20 : 80  2016  \n",
       "2580                    12%           60 : 40  2016  \n",
       "2581                     2%           34 : 66  2016  \n",
       "2582                     9%            9 : 91  2016  \n",
       "2583                    12%           62 : 38  2016  \n",
       "2584                    16%           63 : 37  2016  \n",
       "2585                    10%           61 : 39  2016  \n",
       "2586                     0%           65 : 35  2016  \n",
       "2587                     3%           48 : 52  2016  \n",
       "2588                    22%           53 : 47  2016  \n",
       "2589                     3%           51 : 49  2016  \n",
       "2590                     4%           65 : 35  2016  \n",
       "2591                     3%           34 : 66  2016  \n",
       "2592                     8%           35 : 65  2016  \n",
       "2593                     2%           52 : 48  2016  \n",
       "2594                    15%           53 : 47  2016  \n",
       "2595                     3%           62 : 38  2016  \n",
       "2596                    43%           57 : 43  2016  \n",
       "2597                     2%           29 : 71  2016  \n",
       "2598                     3%           48 : 52  2016  \n",
       "2599                     2%           36 : 64  2016  \n",
       "2600                     3%               NaN  2016  \n",
       "2601                     8%           28 : 72  2016  \n",
       "2602                     4%           43 : 57  2016  \n",
       "\n",
       "[2603 rows x 14 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"world-university-rankings/timesData.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> b) Debido a la estructura será necesario realizar un leve pre-proceso. Existen vacíos entre los datos o valores '-', por lo que será necesario eliminarlos (*o si piensa una mejor manera de manejar ésto puede hacerlo, se verá reflejado en su nota*). Además de ésto deje los datos con *score unkown* o '-' en un conjunto *target* separado, *unlabeled data* (éste será el objetivo del entrenamiento) ¿Cuántos datos quedan en cada conjunto? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertToInt(x):\n",
    "    try:\n",
    "        x = int(x)\n",
    "    except:\n",
    "        x = 0\n",
    "    return x\n",
    "df.dropna(axis=0,inplace=True,how='any') #borra nan\n",
    "df[\"total_score\"] = df[\"total_score\"].apply(lambda x: x.replace('-','unknown')) #rellena \n",
    "df = df[~(df == '-').any(axis=1)] #elimina filas con valores nulos\n",
    "\n",
    "nuevo_df  = pd.get_dummies(df, columns=[\"country\"]) #column to categorical\n",
    "\n",
    "nuevo_df['female'] = nuevo_df['female_male_ratio'].str.split(':', expand=True)[0].apply(convertToInt)\n",
    "nuevo_df['male'] = nuevo_df['female_male_ratio'].str.split(':', expand=True)[1].apply(convertToInt)\n",
    "nuevo_df['female_male_ratio'] =  np.where(nuevo_df['male'] == 0, 0, nuevo_df['female']/nuevo_df['male']) #si no hay (rellena 0) \n",
    "nuevo_df['num_students'] = nuevo_df['num_students'].apply(lambda x: int(str(x).replace(',','')))\n",
    "nuevo_df['international_students'] = nuevo_df['international_students'].apply(lambda x: int(str(x).replace('%','')))\n",
    "#print(nuevo_df.shape)\n",
    "\n",
    "df_test = nuevo_df[nuevo_df[\"total_score\"]=='unknown']  #para predecir al final\n",
    "nuevo_df =  nuevo_df[nuevo_df[\"total_score\"]!='unknown'] #elimina unknown rank.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> c) Cree las matrices de cada conjunto con las que trabajará. Además de ésto separe el conjunto de pruebas fijo que se utilizará, recuerde que éste no puede ser utilizado. Si estima conveniente también cree conjunto de validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = nuevo_df['total_score'].values\n",
    "X = nuevo_df.drop([\"total_score\",\"world_rank\",\"university_name\"],axis=1).values\n",
    "X_test = df_test.drop([\"total_score\",\"world_rank\",\"university_name\"],axis=1).values\n",
    "Y = Y.astype('float32')\n",
    "X = X.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X.shape\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "\n",
    "#validation_set = if you want to create val!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> d) Normalice los datos antes de trabajar. Explique la importancia/conveniencia de realizar ésto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> e) Realice una regresión lineal de mı́nimos cuadrados básica. Mida el residuo de cada predicción en cada dato y haga un gráfico de éste ¿Qué indica lo observado?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jorge\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_axes.py:6462: UserWarning: The 'normed' kwarg is deprecated, and has been replaced by the 'density' kwarg.\n",
      "  warnings.warn(\"The 'normed' kwarg is deprecated, and has been \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1ff58e9c9e8>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression as LR\n",
    "linreg = LR(fit_intercept=True, n_jobs=1)\n",
    "linreg.fit(X_train_scaled,y_train)\n",
    "\n",
    "import seaborn as sns\n",
    "res = y_train-linreg.predict(X_train_scaled)\n",
    "sns.distplot(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> f) Construya una tabla con los pesos, Z-score y F-score correspondientes a cada predictor (variable), compare estos valores. ¿Qué sucede si hacemos un raking de los atributos en base al peso obtenido en la regresión? Compare y comente ¿Qué variables están más correlacionadas con la respuesta? Si usáramos un nivel de significación del 5%. ¿Qué es lo que observa y cuál puede ser la causa?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> g) Calcule la información mútua de los distintos predictores (variables) con respecto a la variable *output* o *target*. Comente con lo calculado anteriormente y se le parece razonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> h)  Construya una función que implemente *Forward Step-wise Selection* (FSS). Es decir, partiendo con un modelo sin predictores (variables), agregue un predictor a la vez, re-ajustando el modelo de regresión en cada paso. Para seleccionar localmente una variable, proponga/implemente un criterio distinto al utilizado en el código de ejemplo. Construya un gráfico que muestre el error de entrenamiento y el error de pruebas como función del número de variables en el modelo. Ordene el eje $x$ de menor a mayor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> i) Ajuste un modelo lineal utilizando “*Ridge Regression*”, es decir, regularizando con la norma $l_2$. Utilice valores del parámetro de regularización $\\lambda$ en el rango [$10^0, 10^6$], variando si estima conveniente. Construya un gráfico que muestre los coeficientes obtenidos como función del parámetro de regularización. Deje un gráfico sólo para analizar los coeficientes de los países. Describa lo que observa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> j) Ajuste un modelo lineal utilizando el método “*Lasso*”, es decir, regularizando con la norma $l_1$. Utilice valores del parámetro de regularización $\\lambda$ en el rango [$10^{-2},10^3$]. Para obtener el código, modifique el ejemplo anterior. Construya un gráfico que muestre los coeficientes obtenidos como función del parámetro de regularización. Describa lo que observa. ¿Es más efectivo *Lasso* para seleccionar atributos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> k) Escogiendo uno de los dos métodos regularizadores anteriores, especificando el porqué, construya un gráfico que muestre el error de entrenamiento y el de pruebas como función del parámetro de regularización. Discuta lo que  observa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> l) Estime el valor del parámetro de regularización en **alguno** de los modelos anteriores haciendo uso de la técnica validación cruzada con un número de folds igual a $K= 5$ y $K = 10$. Recuerde que para que la estimación sea razonable, en cada configuración (*fold*) deberá reajustar los pesos del modelo. Mida el error real del modelo (ésto es sobre el conjunto de pruebas). Debido a la escala del error puede utilizar auxiliarmente *MAE* como métrica de desempeño. Compare y concluya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> m) Con el modelo que se piense que es el mejor, en base a todo lo experimentado. Realice el *ranking* de las universidades del que no se tienen etiquetas (*unlabeled data* o *target data*) a través de predecir los datos que se dejaron como *pruebas* y ordenar su score en el *dataframe*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"tercero\"></a>\n",
    "## 3. Análisis de emociones en *tweets*\n",
    "\n",
    "El análisis de emociones o sentimientos se refiere al proceso de extraer información acerca de la actitud que una persona (o grupo de ellas) manifiesta, en un determinado medio o formato digital, con respecto a un tópico o contexto de comunicación. Uno de los casos más estudiados corresponde a determinar la polaridad de un trozo de texto, es decir, clasificar una determinada evaluación escrita (review ), en que una persona manifiesta una opinión, como positiva, negativa o neutral. Esto también ha sido extendido a otros medios, como lo es analizar la polaridad de textos en redes sociales.\n",
    "\n",
    "<img src=\"https://image.flaticon.com/sprites/new_packs/132222-color-emotions-assets.png\" width=\"40%\" />\n",
    "\n",
    "\n",
    "\n",
    "Para esta actividad se trabajará con un datasets de tweets ofrecidos por CrowdFlower[[8]](#refs). Cada *tweet* está\n",
    "asociado a una emoción en particular, donde el conjunto de emociones se trabajarán como mutuamente excluyentes, siendo un problema de múltiples clases.\n",
    "\n",
    "Los datos pueden ser descargados ejecutando el siguiente código en sistema Unix:\n",
    "```\n",
    "wget https://www.crowdflower.com/wp-content/uploads/2016/07/text_emotion.csv\n",
    "```\n",
    "\n",
    "Para aumentar la eficacia de las caracterı́sticas extraı́das es conveniente ejecutar algunas técnicas de pre-procesamiento básicas.\n",
    "\n",
    "> a) Construya un dataframe con los datos a analizar. Determine cuántas clases existen, cuántos registros por clase y describa el dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     tweet_id   sentiment       author  \\\n",
      "0  1956967341       empty   xoshayzers   \n",
      "1  1956967666     sadness    wannamama   \n",
      "2  1956967696     sadness    coolfunky   \n",
      "3  1956967789  enthusiasm  czareaquino   \n",
      "4  1956968416     neutral    xkilljoyx   \n",
      "\n",
      "                                             content  \n",
      "0  @tiffanylue i know  i was listenin to bad habi...  \n",
      "1  Layin n bed with a headache  ughhhh...waitin o...  \n",
      "2                Funeral ceremony...gloomy friday...  \n",
      "3               wants to hang out with friends SOON!  \n",
      "4  @dannycastillo We want to trade with someone w...  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./emotionanalysis/text_emotion.csv')\n",
    "\n",
    "print (df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El dataset cuenta con cuatro columnas:\n",
    "\n",
    "- tweet_id: Que representa el identificador del tweet.\n",
    "- sentiment: La emoción a la cual están relacionados los tweets.\n",
    "- author: el autor del tweet, indicando su nombre de usuario.\n",
    "- content: el contenido del tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existen  13 clases de sentimientos\n",
      "\n",
      "neutral       8638\n",
      "worry         8459\n",
      "happiness     5209\n",
      "sadness       5165\n",
      "love          3842\n",
      "surprise      2187\n",
      "fun           1776\n",
      "relief        1526\n",
      "hate          1323\n",
      "empty          827\n",
      "enthusiasm     759\n",
      "boredom        179\n",
      "anger          110\n",
      "Name: sentiment, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print (\"Existen \",df.sentiment.value_counts().shape[0], \"clases de sentimientos\\n\")\n",
    "\n",
    "print (df.sentiment.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> b) Construya un conjunto de entrenamiento y otro de pruebas, a través de una máscara aleatoria, para verificar los resultados de los algoritmos. Genere un conjunto de validación si estima conveniente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((31944, 4), (8056, 4))"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msk = np.random.rand(len(df)) < 0.8\n",
    "df_train = df[msk]\n",
    "df_test = df[~msk]\n",
    "df_train.shape,df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> c) Construya las representaciones de los datos con los que trabajará, ya sea para las entradas de los modelos como para las salidas. Recuerde que tendrá que codificar las distintas clases como valores numéricos enteros. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jorge\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:5817: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  regex=regex)\n"
     ]
    }
   ],
   "source": [
    "df_train.replace({'sentiment': {\"neutral\": 0, \"worry\": 1, \"happiness\": 2, \"sadness\": 3, \"love\": 4, \"surprise\": 5, \"fun\": 6, \"relief\": 7, \"hate\": 8, \"empty\": 9, \"enthusiasm\": 10, \"boredom\": 11, \"anger\": 12}},  inplace = True)\n",
    "df_train = df_train.drop([\"tweet_id\",\"author\"],axis=1)\n",
    "\n",
    "#df_test  = pd.get_dummies(df_test, columns=[\"sentiment\"]) #column to categorical\n",
    "df_test.replace({'sentiment': {\"neutral\": 0, \"worry\": 1, \"happiness\": 2, \"sadness\": 3, \"love\": 4, \"surprise\": 5, \"fun\": 6, \"relief\": 7, \"hate\": 8, \"empty\": 9, \"enthusiasm\": 10, \"boredom\": 11, \"anger\": 12}},  inplace = True)\n",
    "df_test = df_test.drop([\"tweet_id\",\"author\"],axis=1)\n",
    "\n",
    "tweet_train = df_train.content\n",
    "sentiment_train = df_train.sentiment\n",
    "\n",
    "tweet_test = df_test.content\n",
    "sentiment_test = df_test.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>Re-pinging @ghostridah14: why didn't you go to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>I should be sleep, but im not! thinking about ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>Hmmm. http://www.djhero.com/ is down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>@charviray Charlene my love. I miss you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>@kelcouch I'm sorry  at least it's Friday?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>Choked on her retainers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3</td>\n",
       "      <td>Ugh! I have to beat this stupid song to get to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3</td>\n",
       "      <td>@BrodyJenner if u watch the hills in london u ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5</td>\n",
       "      <td>Got the news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3</td>\n",
       "      <td>The storm is here and the electricity is gone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4</td>\n",
       "      <td>@annarosekerr agreed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>@PerezHilton lady gaga tweeted about not being...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3</td>\n",
       "      <td>How are YOU convinced that I have always wante...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>6</td>\n",
       "      <td>Wondering why I'm awake at 7am,writing a new s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>I ate Something I don't know what it is... Why...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>3</td>\n",
       "      <td>so tired and i think i'm definitely going to g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>On my way home n having 2 deal w underage girl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>Damm servers still down  i need to hit 80 befo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>3</td>\n",
       "      <td>Fudge.... Just BS'd that whole paper.... So ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>I HATE CANCER. I HATE IT I HATE IT I HATE IT.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>8</td>\n",
       "      <td>It is so annoying when she starts typing on he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0</td>\n",
       "      <td>@cynthia_123 i cant sleep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0</td>\n",
       "      <td>I missed the bl***y bus!!!!!!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0</td>\n",
       "      <td>Screw you @davidbrussee! I only have 3 weeks...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>3</td>\n",
       "      <td>@ether_radio yeah :S i feel all funny cause i ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1</td>\n",
       "      <td>I need skott right now</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0</td>\n",
       "      <td>has work this afternoon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39961</th>\n",
       "      <td>2</td>\n",
       "      <td>Wow! Up, coffee in hand and already outside.  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39962</th>\n",
       "      <td>4</td>\n",
       "      <td>Happy Mothers Day, Mum.. Love you lots   1 mon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39963</th>\n",
       "      <td>2</td>\n",
       "      <td>THE VIDEO IS FINALLY DONE WOOOOOOOOOOOOOOOOOOO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39964</th>\n",
       "      <td>2</td>\n",
       "      <td>Watched Wolverine yesterday ... a spur of the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39965</th>\n",
       "      <td>3</td>\n",
       "      <td>is heading off to the fair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39966</th>\n",
       "      <td>2</td>\n",
       "      <td>the sunset view is SO beautiful from my room!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39968</th>\n",
       "      <td>2</td>\n",
       "      <td>@muffinwomanxo EH! u dont like retro? tisk tis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39969</th>\n",
       "      <td>4</td>\n",
       "      <td>Anyone knows a site like the Swedish site &amp;quo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39970</th>\n",
       "      <td>4</td>\n",
       "      <td>@Rtib happy birthday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39971</th>\n",
       "      <td>10</td>\n",
       "      <td>@SarahSaner Hey Sarah! Hws u? Hope u remember me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39972</th>\n",
       "      <td>2</td>\n",
       "      <td>@acchanosaurus good luck chan! gue kmrn bawa b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39973</th>\n",
       "      <td>6</td>\n",
       "      <td>good morning/midday nation!  FORMULA ONE IN ON...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39975</th>\n",
       "      <td>9</td>\n",
       "      <td>@lexia Or even listen to Susan's green policies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39976</th>\n",
       "      <td>0</td>\n",
       "      <td>right. coursework now. PROMISE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39979</th>\n",
       "      <td>5</td>\n",
       "      <td>@prinsezha awesome. Wha'dya get her?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39980</th>\n",
       "      <td>2</td>\n",
       "      <td>Sitting in Gatwick- going home for a week! can...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39981</th>\n",
       "      <td>2</td>\n",
       "      <td>@maynaseric good luck with your auction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39982</th>\n",
       "      <td>0</td>\n",
       "      <td>hey guys, if you have something to ask, just a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39983</th>\n",
       "      <td>0</td>\n",
       "      <td>@Astronick not really just leaving flat now, o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39984</th>\n",
       "      <td>5</td>\n",
       "      <td>@iscreamshinki Oh that's why.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39987</th>\n",
       "      <td>2</td>\n",
       "      <td>gave the bikes a thorough wash, degrease it an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39988</th>\n",
       "      <td>2</td>\n",
       "      <td>had SUCH and AMAZING time last night, McFly we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39989</th>\n",
       "      <td>4</td>\n",
       "      <td>His snoring is so annoying n it keeps me from ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39991</th>\n",
       "      <td>0</td>\n",
       "      <td>@lovelylisaj can you give me the link for the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39992</th>\n",
       "      <td>0</td>\n",
       "      <td>@jasimmo Ooo showing of your French skills!! l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39994</th>\n",
       "      <td>2</td>\n",
       "      <td>Succesfully following Tayla!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39995</th>\n",
       "      <td>0</td>\n",
       "      <td>@JohnLloydTaylor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39996</th>\n",
       "      <td>4</td>\n",
       "      <td>Happy Mothers Day  All my love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39997</th>\n",
       "      <td>4</td>\n",
       "      <td>Happy Mother's Day to all the mommies out ther...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39999</th>\n",
       "      <td>4</td>\n",
       "      <td>@mopedronin bullet train from tokyo    the gf ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31944 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentiment                                            content\n",
       "0              9  @tiffanylue i know  i was listenin to bad habi...\n",
       "1              3  Layin n bed with a headache  ughhhh...waitin o...\n",
       "2              3                Funeral ceremony...gloomy friday...\n",
       "5              1  Re-pinging @ghostridah14: why didn't you go to...\n",
       "6              3  I should be sleep, but im not! thinking about ...\n",
       "7              1               Hmmm. http://www.djhero.com/ is down\n",
       "8              3            @charviray Charlene my love. I miss you\n",
       "9              3         @kelcouch I'm sorry  at least it's Friday?\n",
       "11             1                            Choked on her retainers\n",
       "12             3  Ugh! I have to beat this stupid song to get to...\n",
       "13             3  @BrodyJenner if u watch the hills in london u ...\n",
       "14             5                                       Got the news\n",
       "15             3      The storm is here and the electricity is gone\n",
       "16             4                               @annarosekerr agreed\n",
       "18             1  @PerezHilton lady gaga tweeted about not being...\n",
       "19             3  How are YOU convinced that I have always wante...\n",
       "21             6  Wondering why I'm awake at 7am,writing a new s...\n",
       "23             1  I ate Something I don't know what it is... Why...\n",
       "24             3  so tired and i think i'm definitely going to g...\n",
       "25             1  On my way home n having 2 deal w underage girl...\n",
       "27             1  Damm servers still down  i need to hit 80 befo...\n",
       "28             3  Fudge.... Just BS'd that whole paper.... So ti...\n",
       "29             1      I HATE CANCER. I HATE IT I HATE IT I HATE IT.\n",
       "30             8  It is so annoying when she starts typing on he...\n",
       "31             0                          @cynthia_123 i cant sleep\n",
       "32             0                    I missed the bl***y bus!!!!!!!!\n",
       "35             0    Screw you @davidbrussee! I only have 3 weeks...\n",
       "36             3  @ether_radio yeah :S i feel all funny cause i ...\n",
       "37             1                             I need skott right now\n",
       "38             0                            has work this afternoon\n",
       "...          ...                                                ...\n",
       "39961          2  Wow! Up, coffee in hand and already outside.  ...\n",
       "39962          4  Happy Mothers Day, Mum.. Love you lots   1 mon...\n",
       "39963          2  THE VIDEO IS FINALLY DONE WOOOOOOOOOOOOOOOOOOO...\n",
       "39964          2  Watched Wolverine yesterday ... a spur of the ...\n",
       "39965          3                         is heading off to the fair\n",
       "39966          2      the sunset view is SO beautiful from my room!\n",
       "39968          2  @muffinwomanxo EH! u dont like retro? tisk tis...\n",
       "39969          4  Anyone knows a site like the Swedish site &quo...\n",
       "39970          4                               @Rtib happy birthday\n",
       "39971         10   @SarahSaner Hey Sarah! Hws u? Hope u remember me\n",
       "39972          2  @acchanosaurus good luck chan! gue kmrn bawa b...\n",
       "39973          6  good morning/midday nation!  FORMULA ONE IN ON...\n",
       "39975          9    @lexia Or even listen to Susan's green policies\n",
       "39976          0                     right. coursework now. PROMISE\n",
       "39979          5               @prinsezha awesome. Wha'dya get her?\n",
       "39980          2  Sitting in Gatwick- going home for a week! can...\n",
       "39981          2            @maynaseric good luck with your auction\n",
       "39982          0  hey guys, if you have something to ask, just a...\n",
       "39983          0  @Astronick not really just leaving flat now, o...\n",
       "39984          5                      @iscreamshinki Oh that's why.\n",
       "39987          2  gave the bikes a thorough wash, degrease it an...\n",
       "39988          2  had SUCH and AMAZING time last night, McFly we...\n",
       "39989          4  His snoring is so annoying n it keeps me from ...\n",
       "39991          0  @lovelylisaj can you give me the link for the ...\n",
       "39992          0  @jasimmo Ooo showing of your French skills!! l...\n",
       "39994          2                      Succesfully following Tayla!!\n",
       "39995          0                                   @JohnLloydTaylor\n",
       "39996          4                     Happy Mothers Day  All my love\n",
       "39997          4  Happy Mother's Day to all the mommies out ther...\n",
       "39999          4  @mopedronin bullet train from tokyo    the gf ...\n",
       "\n",
       "[31944 rows x 2 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> d) Entrene y compare al menos 4 de los diferentes clasificadores vistos en clases para clasificación (por ejemplo: Navie Bayes, Multinomial Naive Bayes, LDA, QDA, Regresión logı́stica y Perceptrón). Recuerde que algunos son extendidos por defecto a múltiples clases para detectar emociones en cada *tweet*, sin embargo, otros deben ser extentidos a través de otras técnicas, tal como *One vs One* y *One vs All/Rest*. Muestre tabla o gráfico resumen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 1), binary='False')\n",
    "vectorizer.fit(np.asarray(tweet_train))\n",
    "\n",
    "X_train = vectorizer.transform(tweet_train)\n",
    "X_test = vectorizer.transform(tweet_test)\n",
    "\n",
    "y_train = np.asarray(sentiment_train.astype(float))\n",
    "y_test = np.asarray(sentiment_test.astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression as LR\n",
    "model = LR(fit_intercept=True, normalize=False)\n",
    "model = model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE on train:  0.34068379829850276\n",
      "MAE on validation:  4.4969163955924225\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error #measure MAE\n",
    "print(\"MAE on train: \",mean_absolute_error(y_train, model.predict(X_train)))\n",
    "print(\"MAE on validation: \",mean_absolute_error(y_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> e) Utilice la técnica de ECOC (*Error-Correcting Output-Code*) para extender a multiclases algunos de los clasificadores utilizados en d). Comente lo que hace la técnica y los resultados observados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> f) Evalúe la métrica de *accuracy* sobre el conjunto de pruebas del mejor clasificador encontrado.  \n",
    "*Recuerde que puede acudir a otras métricas para tener otras visiones de lo que está haciendo el modelo de aprendizaje*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> g) Intente mejorar su resultado considerablemente a través de alguna mejora novedosa. Se espera que supere el 35% de *accuracy*.\n",
    "\n",
    "<div class=\"alert alert-warning\"> Una opción es cambiar considerablemente la representación de los textos, ya sea con Tf-Idf, word2vec[[9]](#refs) , doc2vec[[10]](#refs) , otros. </div>\n",
    "\n",
    "<div class=\"alert alert-warning\"> Otra opción es hacer una clasificación por grupos, es decir, agrupar emociones para ir distinguiendo y bajar la granulidad de la clasificación. Como una clasificación jerárquica en modo árbol.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
